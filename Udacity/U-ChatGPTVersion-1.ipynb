{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1f29c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, TypedDict, Literal, Any, Dict\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from tavily import TavilyClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fb92965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------- Configuration -------------\n",
    "REPUTABLE_DOMAINS = [\n",
    "    \"nhs.uk\", \"who.int\", \"cdc.gov\", \"medlineplus.gov\", \"nih.gov\",\n",
    "    \"mayoclinic.org\", \"cancer.gov\", \"nice.org.uk\", \"ema.europa.eu\"\n",
    "]\n",
    "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")  # pick your model\n",
    "TEMPERATURE = float(os.getenv(\"OPENAI_TEMPERATURE\", \"0.2\"))\n",
    "MAX_RESULTS = 5\n",
    "\n",
    "# ------------- LLM -------------\n",
    "llm = ChatOpenAI(model=MODEL, temperature=TEMPERATURE)\n",
    "\n",
    "# ------------- Tavily -------------\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "# ------------- State Schema -------------\n",
    "class HealthState(TypedDict, total=False):\n",
    "    topic: Optional[str]\n",
    "    search_results: Optional[List[Dict[str, Any]]]\n",
    "    corpus: Optional[str]          # concatenated full-text extracts\n",
    "    summary: Optional[str]\n",
    "    quiz_question: Optional[str]\n",
    "    correct_answer: Optional[str]\n",
    "    user_answer: Optional[str]\n",
    "    grade: Optional[str]           # e.g., \"Correct\" / \"Incorrect\"\n",
    "    feedback: Optional[str]        # must include citations\n",
    "    next_action: Optional[Literal[\n",
    "        \"ask_topic\",\n",
    "        \"search\",\n",
    "        \"summarize\",\n",
    "        \"show_summary_and_wait\",\n",
    "        \"generate_quiz\",\n",
    "        \"present_quiz_and_wait\",\n",
    "        \"grade_quiz\",\n",
    "        \"present_grade\",\n",
    "        \"decide_continue\",\n",
    "        \"reset_or_exit\"\n",
    "    ]]\n",
    "    # For CLI driving/human-in-the-loop:\n",
    "    human_input: Optional[str]\n",
    "    continue_flag: Optional[bool]  # True to start over; False to exit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eebf193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------- Helper Prompts -------------\n",
    "SAFETY_PREAMBLE = \"\"\"\\\n",
    "You are a patient education assistant. Follow these rules:\n",
    "- Provide general, patient-friendly information only. Do NOT give medical advice, diagnosis, or treatment recommendations.\n",
    "- Urge patients to consult a qualified clinician for personal medical concerns or emergencies.\n",
    "- Use plain language. Explain acronyms on first use.\n",
    "- Cite sources clearly with bracketed references like [NHS], [CDC], [WHO], [NIH], [MayoClinic], etc.\n",
    "\"\"\"\n",
    "\n",
    "SUMMARIZE_PROMPT = \"\"\"\\\n",
    "{preamble}\n",
    "Summarize the key information on the topic: \"{topic}\" using ONLY the following source extracts.\n",
    "Write for a UK patient audience (plain English, neutral tone). Include a short bullet list of key takeaways, and clearly list sources at the end in square brackets.\n",
    "Avoid advice: if a section implies treatment or diagnosis, state that the patient should consult a clinician.\n",
    "\n",
    "SOURCE EXTRACTS:\n",
    "{corpus}\n",
    "\"\"\"\n",
    "\n",
    "QUIZ_PROMPT = \"\"\"\\\n",
    "{preamble}\n",
    "Create ONE multiple-choice comprehension question (A–D) based ONLY on the summary below.\n",
    "- Make it directly test a key fact.\n",
    "- Provide the correct answer (A–D) and a one-sentence rationale.\n",
    "\n",
    "SUMMARY:\n",
    "{summary}\n",
    "\n",
    "Return JSON with fields: question, choices (list of 4), correct_letter, rationale.\n",
    "\"\"\"\n",
    "\n",
    "GRADE_PROMPT = \"\"\"\\\n",
    "{preamble}\n",
    "Grade the patient's answer to the quiz based ONLY on the summary below.\n",
    "- Respond \"Correct\" or \"Incorrect\".\n",
    "- Provide 2–4 sentences of feedback that cite the summary with inline bracketed source tags (e.g., [NHS], [CDC]) that appear IN THE SUMMARY TEXT.\n",
    "- If the summary does not support grading, say so.\n",
    "\n",
    "SUMMARY:\n",
    "{summary}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CORRECT ANSWER: {correct_letter}\n",
    "PATIENT ANSWER: {user_answer}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------- Nodes -------------\n",
    "def ask_topic(state: HealthState) -> HealthState:\n",
    "    # This node waits for human_input externally (CLI driver or UI).\n",
    "    topic = state.get(\"human_input\")\n",
    "    out: HealthState = {\"next_action\": \"search\"}\n",
    "    if topic:\n",
    "        out[\"topic\"] = topic.strip()\n",
    "    return out\n",
    "\n",
    "def search_tavily(state: HealthState) -> HealthState:\n",
    "    topic = state[\"topic\"]\n",
    "    # Prefer reputable domains using Tavily's domain filter and textual query\n",
    "    query = f\"{topic} site:({' OR '.join(REPUTABLE_DOMAINS)})\"\n",
    "    # Tavily 'search' for links\n",
    "    results = tavily.search(query=query, include_raw_content=False, max_results=MAX_RESULTS)\n",
    "    # Retrieve full page text for each result (best-effort: skip failures)\n",
    "    corpus_parts = []\n",
    "    curated_results = []\n",
    "    for r in results.get(\"results\", [])[:MAX_RESULTS]:\n",
    "        url = r.get(\"url\")\n",
    "        if not url:\n",
    "            continue\n",
    "        # filter by domain whitelist\n",
    "        if not any(d in url for d in REPUTABLE_DOMAINS):\n",
    "            continue\n",
    "        try:\n",
    "            page = tavily.get_page_content(url)\n",
    "            content = page.get(\"content\", \"\") or \"\"\n",
    "            if content.strip():\n",
    "                # keep a concise chunk to avoid context overflow\n",
    "                snippet = content[:6000]\n",
    "                corpus_parts.append(f\"URL: {url}\\n\\n{snippet}\")\n",
    "                curated_results.append({\"url\": url, \"title\": r.get(\"title\", \"\")})\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        \"search_results\": curated_results,\n",
    "        \"corpus\": \"\\n\\n---\\n\\n\".join(corpus_parts),\n",
    "        \"next_action\": \"summarize\",\n",
    "    }\n",
    "\n",
    "def summarize(state: HealthState) -> HealthState:\n",
    "    corpus = state.get(\"corpus\") or \"\"\n",
    "    topic = state[\"topic\"]\n",
    "    if not corpus.strip():\n",
    "        # Fall back on a safer message if nothing fetched\n",
    "        summary = (\n",
    "            f\"{SAFETY_PREAMBLE}\\n\\n\"\n",
    "            \"I couldn’t retrieve reliable sources just now. Please try a different phrasing \"\n",
    "            \"or consult trusted sites like NHS, WHO, or CDC.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = SUMMARIZE_PROMPT.format(\n",
    "            preamble=SAFETY_PREAMBLE, topic=topic, corpus=corpus\n",
    "        )\n",
    "        summary = llm.invoke(prompt).content\n",
    "\n",
    "    return {\"summary\": summary, \"next_action\": \"show_summary_and_wait\"}\n",
    "\n",
    "def show_summary_and_wait(state: HealthState) -> HealthState:\n",
    "    # This is a pause point for the UI to display summary and wait for user \"ready\".\n",
    "    # Driver should set human_input to e.g. \"ready\" to continue.\n",
    "    return {\"next_action\": \"generate_quiz\"}\n",
    "\n",
    "def generate_quiz(state: HealthState) -> HealthState:\n",
    "    summary = state[\"summary\"]\n",
    "    prompt = QUIZ_PROMPT.format(preamble=SAFETY_PREAMBLE, summary=summary)\n",
    "    resp = llm.invoke(prompt).content\n",
    "    # simple JSON extraction (be defensive)\n",
    "    import json\n",
    "    try:\n",
    "        data = json.loads(resp)\n",
    "        question = data[\"question\"]\n",
    "        choices = data[\"choices\"]\n",
    "        correct_letter = data[\"correct_letter\"].strip().upper()\n",
    "        rationale = data.get(\"rationale\", \"\")\n",
    "        # bake choices into the question text for easy display\n",
    "        q_text = question + \"\\n\" + \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)])\n",
    "        return {\"quiz_question\": q_text, \"correct_answer\": correct_letter,\n",
    "                \"next_action\": \"present_quiz_and_wait\"}\n",
    "    except Exception:\n",
    "        # Fallback: basic dummy if parsing failed\n",
    "        fallback_q = \"Which organization is known for UK public health guidance?\\nA. \" \\\n",
    "        \"NASA\\nB. NHS\\nC. IMF\\nD. FIFA\"\n",
    "        return {\"quiz_question\": fallback_q, \"correct_answer\": \"B\",\n",
    "                \"next_action\": \"present_quiz_and_wait\"}\n",
    "\n",
    "def present_quiz_and_wait(state: HealthState) -> HealthState:\n",
    "    # Pause for user's answer (driver sets human_input to 'A'/'B'/'C'/'D')\n",
    "    return {\"next_action\": \"grade_quiz\"}\n",
    "\n",
    "def grade_quiz(state: HealthState) -> HealthState:\n",
    "    summary = state[\"summary\"]\n",
    "    question = state[\"quiz_question\"]\n",
    "    correct_letter = (state[\"correct_answer\"] or \"\").strip().upper()\n",
    "    user_answer = (state.get(\"human_input\") or \"\").strip().upper()\n",
    "\n",
    "    prompt = GRADE_PROMPT.format(\n",
    "        preamble=SAFETY_PREAMBLE,\n",
    "        summary=summary,\n",
    "        question=question,\n",
    "        correct_letter=correct_letter,\n",
    "        user_answer=user_answer or \"N/A\",\n",
    "    )\n",
    "    resp = llm.invoke(prompt).content\n",
    "\n",
    "    # Heuristic: grade line then feedback body\n",
    "    grade = \"Correct\" if user_answer == correct_letter else \"Incorrect\"\n",
    "    feedback = resp\n",
    "    return {\"user_answer\": user_answer, \"grade\": grade,\n",
    "            \"feedback\": feedback, \"next_action\": \"present_grade\"}\n",
    "\n",
    "def present_grade(state: HealthState) -> HealthState:\n",
    "    # UI shows grade + feedback; then flow moves to decide_continue\n",
    "    return {\"next_action\": \"decide_continue\"}\n",
    "\n",
    "def decide_continue(state: HealthState) -> HealthState:\n",
    "    # Driver should set continue_flag True/False based on user input\n",
    "    return {\"next_action\": \"reset_or_exit\"}\n",
    "\n",
    "def reset_or_exit(state: HealthState) -> HealthState:\n",
    "    if state.get(\"continue_flag\"):\n",
    "        # Reset privacy-sensitive fields before looping\n",
    "        return {\n",
    "            \"topic\": None,\n",
    "            \"search_results\": None,\n",
    "            \"corpus\": None,\n",
    "            \"summary\": None,\n",
    "            \"quiz_question\": None,\n",
    "            \"correct_answer\": None,\n",
    "            \"user_answer\": None,\n",
    "            \"grade\": None,\n",
    "            \"feedback\": None,\n",
    "            \"human_input\": None,\n",
    "            \"next_action\": \"ask_topic\",\n",
    "        }\n",
    "    else:\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "001ac289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2c88d011090>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ------------- Build the Graph -------------\n",
    "graph = StateGraph(HealthState)\n",
    "graph.add_node(\"ask_topic\", ask_topic)\n",
    "graph.add_node(\"search\", search_tavily)\n",
    "graph.add_node(\"summarize\", summarize)\n",
    "graph.add_node(\"show_summary_and_wait\", show_summary_and_wait)\n",
    "graph.add_node(\"generate_quiz\", generate_quiz)\n",
    "graph.add_node(\"present_quiz_and_wait\", present_quiz_and_wait)\n",
    "graph.add_node(\"grade_quiz\", grade_quiz)\n",
    "graph.add_node(\"present_grade\", present_grade)\n",
    "graph.add_node(\"decide_continue\", decide_continue)\n",
    "graph.add_node(\"reset_or_exit\", reset_or_exit)\n",
    "\n",
    "# Edges\n",
    "graph.add_edge(\"ask_topic\", \"search\")\n",
    "graph.add_edge(\"search\", \"summarize\")\n",
    "graph.add_edge(\"summarize\", \"show_summary_and_wait\")\n",
    "graph.add_edge(\"show_summary_and_wait\", \"generate_quiz\")\n",
    "graph.add_edge(\"generate_quiz\", \"present_quiz_and_wait\")\n",
    "graph.add_edge(\"present_quiz_and_wait\", \"grade_quiz\")\n",
    "graph.add_edge(\"grade_quiz\", \"present_grade\")\n",
    "graph.add_edge(\"present_grade\", \"decide_continue\")\n",
    "graph.add_edge(\"decide_continue\", \"reset_or_exit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b14efb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Repeat or end:\n",
    "def router(state: HealthState) -> str:\n",
    "    nxt = state.get(\"next_action\")\n",
    "    return nxt if nxt else END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a975428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph.set_entry_point(\"ask_topic\")\n",
    "app = graph.compile(checkpointer=MemorySaver(),\n",
    " #                   interrupt_before=[\"ask_topic\", \"show_summary_and_wait\",\n",
    "  #                                    \"present_quiz_and_wait\", \"decide_continue\"]\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71bc5a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HealthBot (Patient Education) ===\n",
      "Note: Information only. Not medical advice. For concerns, contact a clinician.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Checkpointer requires one or more of the following 'configurable' keys: thread_id, checkpoint_ns, checkpoint_id",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[43mrun_cli\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mrun_cli\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     33\u001b[39m     state[\u001b[33m\"\u001b[39m\u001b[33mcontinue_flag\u001b[39m\u001b[33m\"\u001b[39m] = (again == \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Advance graph\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m state = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Exit condition\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state.get(\u001b[33m\"\u001b[39m\u001b[33mnext_action\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\PythonProjects\\Project01\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\PythonProjects\\Project01\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2529\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2512\u001b[39m run_manager = callback_manager.on_chain_start(\n\u001b[32m   2513\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2514\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2515\u001b[39m     name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.get_name()),\n\u001b[32m   2516\u001b[39m     run_id=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   2517\u001b[39m )\n\u001b[32m   2518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2519\u001b[39m     \u001b[38;5;66;03m# assign defaults\u001b[39;00m\n\u001b[32m   2520\u001b[39m     (\n\u001b[32m   2521\u001b[39m         stream_modes,\n\u001b[32m   2522\u001b[39m         output_keys,\n\u001b[32m   2523\u001b[39m         interrupt_before_,\n\u001b[32m   2524\u001b[39m         interrupt_after_,\n\u001b[32m   2525\u001b[39m         checkpointer,\n\u001b[32m   2526\u001b[39m         store,\n\u001b[32m   2527\u001b[39m         cache,\n\u001b[32m   2528\u001b[39m         durability_,\n\u001b[32m-> \u001b[39m\u001b[32m2529\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2532\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2533\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2534\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2535\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2536\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2538\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m checkpointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m durability \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2539\u001b[39m         warnings.warn(\n\u001b[32m   2540\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`durability` has no effect when no checkpointer is present.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2541\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\PythonProjects\\Project01\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2401\u001b[39m, in \u001b[36mPregel._defaults\u001b[39m\u001b[34m(self, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability)\u001b[39m\n\u001b[32m   2399\u001b[39m     checkpointer = \u001b[38;5;28mself\u001b[39m.checkpointer\n\u001b[32m   2400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpointer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.get(CONF):\n\u001b[32m-> \u001b[39m\u001b[32m2401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2402\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheckpointer requires one or more of the following \u001b[39m\u001b[33m'\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2403\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkeys: thread_id, checkpoint_ns, checkpoint_id\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2404\u001b[39m     )\n\u001b[32m   2405\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m CONFIG_KEY_RUNTIME \u001b[38;5;129;01min\u001b[39;00m config.get(CONF, {}):\n\u001b[32m   2406\u001b[39m     store: BaseStore | \u001b[38;5;28;01mNone\u001b[39;00m = config[CONF][CONFIG_KEY_RUNTIME].store\n",
      "\u001b[31mValueError\u001b[39m: Checkpointer requires one or more of the following 'configurable' keys: thread_id, checkpoint_ns, checkpoint_id"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------- Minimal CLI Driver (demo) -------------\n",
    "def run_cli():\n",
    "    print(\"=== HealthBot (Patient Education) ===\")\n",
    "    print(\"Note: Information only. Not medical advice. For concerns, contact a clinician.\\n\")\n",
    "\n",
    "    state: HealthState = {\"next_action\": \"ask_topic\"}\n",
    "    while True:\n",
    "        step = state.get(\"next_action\", \"ask_topic\")\n",
    "\n",
    "        if step == \"ask_topic\":\n",
    "            topic = input(\"What health topic or condition would you like to learn about? \")\n",
    "            state[\"human_input\"] = topic\n",
    "\n",
    "        elif step == \"show_summary_and_wait\":\n",
    "            print(\"\\n--- Summary ---\")\n",
    "            print(state.get(\"summary\") or \"(No summary)\")\n",
    "            input(\"\\nPress Enter when you’re ready for a quick comprehension check...\")\n",
    "\n",
    "        elif step == \"present_quiz_and_wait\":\n",
    "            print(\"\\n--- Quiz ---\")\n",
    "            print(state.get(\"quiz_question\") or \"(No quiz)\")\n",
    "            ans = input(\"Enter your answer (A/B/C/D): \").strip().upper()\n",
    "            state[\"human_input\"] = ans\n",
    "\n",
    "        elif step == \"present_grade\":\n",
    "            print(\"\\n--- Your Result ---\")\n",
    "            print(f\"Grade: {state.get('grade')}\")\n",
    "            print(\"Feedback:\\n\" + (state.get(\"feedback\") or \"\"))\n",
    "            # fall through to decide_continue\n",
    "\n",
    "        elif step == \"decide_continue\":\n",
    "            again = input(\"\\nWould you like to learn about another topic? (y/n): \").strip().lower()\n",
    "            state[\"continue_flag\"] = (again == \"y\")\n",
    "\n",
    "        # Advance graph\n",
    "        state = app.invoke(state)\n",
    "\n",
    "        # Exit condition\n",
    "        if state.get(\"next_action\") is None:\n",
    "            print(\"\\nThanks for using HealthBot. Take care!\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_cli()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
